{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"KCP contrib contents","text":"<p>This repository contains the list of demo code, slides and other materials used in the meetups, conferences.</p> <p>For sessions walkthrough with step-by-step exercises, visit the Learning page.</p> <ul> <li>Platform Engineering Day EU 2024 - Building a Platform Engineering API Layer with kcp<ul> <li>YouTube</li> <li>Slides</li> </ul> </li> <li>KCP ML shop - KCP Service owner, provider and user demo.<ul> <li>YouTube</li> <li>Slides</li> </ul> </li> <li>KubeCon'24 Salt Lake City, US -  Deep Dive Into Generic Control Planes and kcp<ul> <li>YouTube</li> <li>Slides</li> <li>Demo</li> </ul> </li> </ul>"},{"location":"#other-content","title":"Other content","text":"<ul> <li>KubeCon 2024 - Paris - Why Kubernetes Is Inappropriate for Platforms and How to Make It Better (video)</li> <li>KubeCon EU 2021: Kubernetes as the Hybrid Cloud Control Plane Keynote - Clayton Coleman (video)</li> <li>OpenShift Commons: Kubernetes as the Control Plane for the Hybrid Cloud - Clayton Coleman (video)</li> <li>TGI Kubernetes 157: Exploring kcp: apiserver without Kubernetes</li> <li>K8s SIG Architecture meeting discussing kcp - June 29, 2021</li> <li>Let's Learn kcp - A minimal Kubernetes API server with Saiyam Pathak - July 7, 2021</li> </ul>"},{"location":"examples/oidc/readme/","title":"Configuring KCP for OIDC","text":"<p>KCP fully supports OIDC authentication and authorization as supported by kubernetes' apiserver. The full documentation is available here.</p> <p>This example uses dex as an OIDC provider and demonstrates how to configure KCP to authenticate against it.</p>"},{"location":"examples/oidc/readme/#prerequisites","title":"Prerequisites","text":"<ul> <li>docker</li> <li>jq</li> </ul> <p>Clone the contrib repository and enter the <code>examples/oidc</code> directory.</p>"},{"location":"examples/oidc/readme/#create-network","title":"Create network","text":"<p>Create a separate docker network for the kcp components to run in:</p> <pre><code>docker network create kcp\n</code></pre>"},{"location":"examples/oidc/readme/#setup-dex-idp","title":"Setup dex idp","text":"<p>Generate certificates for dex - while dex can be used without TLS, kube authentication requires the use of TLS certificates:</p> <pre><code>openssl req -x509 -newkey rsa:4096 \\\n    -keyout dex/server.key \\\n    -out dex/server.crt \\\n    -passout pass: \\\n    -sha256 \\\n    -days 3650 \\\n    -nodes \\\n    -subj \"/C=XX/ST=XX/L=XX/O=XX/OU=XX/CN=dex\" \\\n    -addext \"subjectAltName = DNS:dex\"\n</code></pre> <p>Run dex in docker:</p> <pre><code>docker run --network kcp --detach --rm --name dex \\\n    -v ./dex:/dex:ro \\\n    ghcr.io/dexidp/dex:latest \\\n    dex serve /dex/dex-config.yaml\n</code></pre> <p>The configuration configures one static user <code>admin@example.com</code> with the password <code>admin</code> and a client <code>kcp</code>.</p> <p>Validate functionality with oidc-login:</p> <pre><code>docker run --network=kcp --rm -v ./dex/server.crt:/dex.crt  \\\n    ghcr.io/int128/kubelogin:master \\\n    setup \\\n    --oidc-issuer-url=https://dex:5557/ \\\n    --oidc-client-id=kcp \\\n    --certificate-authority=/dex.crt \\\n    --grant-type=password \\\n    --username=admin@example.com \\\n    --password=admin\n</code></pre>"},{"location":"examples/oidc/readme/#setup-kcp","title":"Setup kcp","text":""},{"location":"examples/oidc/readme/#authentication-configuration","title":"Authentication configuration","text":"<p>Create a simple authentication config:</p> <pre><code>cat &lt;&lt;EOF &gt; authentication-config.yaml\napiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt:\n- issuer:\n    url: https://dex:5557/\n    certificateAuthority: |-\n$(awk '{ print \"      \" $0 }' dex/server.crt)\n    audiences:\n      - kcp\n  claimValidationRules:\n  # This validation is required to use claims.email in claimMappings.\n  - expression: 'claims.email_verified == true'\n    message: email_verified must be set to true\n  claimMappings:\n    username:\n      expression: \"claims.email + ':external-user'\"\n    groups:\n      claim: roles\n      prefix: \"oidc:\"\nEOF\n</code></pre> <p>The <code>claimMappings</code> instructs the apiserver to recognize the user <code>admin@example.com</code> as <code>admin@example.com:external-user</code> and to assign groups based on the <code>roles</code> claim, prefixed with <code>oidc:</code>.</p> <p>A sensible mapping is to have an id for each authentication provider and to set the prefix to this id to prevent RBAC oversights.</p> <pre><code>apiVersion: apiserver.config.k8s.io/v1beta1\nkind: AuthenticationConfiguration\njwt:\n- issuer:\n    url: https://auth.example.corp/\n    # ...\n  claimMappings:\n    username:\n      claim: sub\n      prefix: \"auth-example-corp:\"\n    groups:\n      claim: roles\n      prefix: \"auth-example-corp:\"\n- issuer:\n    url: https://auth.supplier.org/\n    # ...\n  claimMappings:\n    username:\n      claim: sub\n      prefix: \"auth-supplier-org:\"\n    groups:\n      claim: roles\n      prefix: \"auth-supplier-org:\"\n</code></pre>"},{"location":"examples/oidc/readme/#run-kcp","title":"Run KCP","text":"<p>Now launch KCP with the authentication config:</p> <pre><code>docker run --network kcp --rm --detach --name kcp \\\n    -p 6443:6443 \\\n    -v ./authentication-config.yaml:/authentication-config.yaml:ro \\\n    ghcr.io/kcp-dev/kcp:main \\\n    start \\\n    --bind-address=0.0.0.0 \\\n    --external-hostname=localhost \\\n    --authentication-config=/authentication-config.yaml\n</code></pre> <p>KCP can take around 30s to start up, so we wait until we can copy the admin kubeconfig from the container and then wait for the default namespace to become active:</p> <pre><code>while ! docker exec kcp test -f /.kcp/admin.kubeconfig; do sleep 1; done\ndocker cp kcp:/.kcp/admin.kubeconfig admin.kubeconfig\nkubectl --kubeconfig=admin.kubeconfig wait --for=jsonpath='{.status.phase}'=Active namespace default\n</code></pre> <p>Use the admin kubeconfig to create a configmap in the root workspace to read as an oidc user later:</p> <pre><code>kubectl --kubeconfig=admin.kubeconfig create configmap hello-world \\\n    --from-literal=message=\"Hello, KCP with OIDC\"\n</code></pre>"},{"location":"examples/oidc/readme/#authenticating-with-oidc","title":"Authenticating with OIDC","text":"<p>With a local setup with dex the three-step process of SSO between app, auth provider and user is not possible, so we will simulate it by getting a token from dex using kubelogin and then using that token to authenticate with KCP.</p> <p>We create a new kubeconfig for oidc <code>oidc.kubeconfig.yaml</code>, for that we will also need the certificate of the KCP apiserver:</p> <pre><code>docker cp kcp:/.kcp/apiserver.crt apiserver.crt\nkubectl --kubeconfig oidc.kubeconfig.yaml config set-cluster kcp \\\n    --server https://localhost:6443/clusters/root \\\n    --certificate-authority=apiserver.crt\n</code></pre> <p>Get a token from dex using kubelogin and store it in <code>oidc.token</code>:</p> <pre><code>docker run --rm --network kcp -v ./dex/server.crt:/dex.crt  \\\n    ghcr.io/int128/kubelogin:master \\\n    get-token \\\n    --oidc-issuer-url=https://dex:5557/ \\\n    --oidc-client-id=kcp \\\n    --oidc-extra-scope=email \\\n    --certificate-authority=/dex.crt \\\n    --grant-type=password \\\n    --username=admin@example.com \\\n    --password=admin \\\n    | jq -r '.status.token' &gt; oidc.token\n</code></pre> <p>And set the token in the credentials of the kubeconfig - this is basically what kubectl does behind the scenes:</p> <pre><code>kubectl --kubeconfig oidc.kubeconfig.yaml config set-credentials kcp-oidc \\\n   --auth-provider=oidc \\\n   --auth-provider-arg=idp-issuer-url=https://dex:5557/ \\\n   --auth-provider-arg=client-id=kcp \\\n   --auth-provider-arg=idp-certificate-authority=dex/server.crt \\\n   --token=$(cat oidc.token)\n</code></pre> <p>The <code>idp-issuer-url</code> is the same as in the <code>AuthenticationConfiguration</code>. The apiserver will use this to validate our token against the oidc provider and to use the correct claims mapping.</p> <p>Usually a kubectl config with oidc would look like this:</p> <pre><code>apiVersion: v1\nkind: Config\n# ...\nusers:\n  - name: admin@example.com\n    user:\n      exec:\n        apiVersion: client.authentication.k8s.io/v1\n        interactiveMode: Never\n        command: kubectl\n        args:\n          - oidc-login\n          - get-token\n          - --oidc-issuer-url=https://dex:5557/\n          - --oidc-client-id=kcp\n          - --oidc-extra-scope=email\n</code></pre> <p>What we configured above is what <code>kubectl oidc-login get-token</code> would output as JSON.</p> <p>And now to bind it together:</p> <pre><code>kubectl --kubeconfig oidc.kubeconfig.yaml config set-context kcp-oidc \\\n    --cluster=kcp \\\n    --user=kcp-oidc\n\nkubectl --kubeconfig oidc.kubeconfig.yaml config use-context kcp-oidc\n</code></pre> <p>With the oidc kubeconfig built we can now access the KCP cluster as an oidc user:</p> <p>```bash noci kubectl --kubeconfig oidc.kubeconfig.yaml get configmap hello-world <pre><code>And we run into a permission issue - just because a user is\nauthenticated does not mean they are also authorized.\n\n## RBAC with OIDC\n\nTo authorize users we use the same RBAC rules as with any other kube\nuser or group. In the authentication config we suffixed the user's email\nwith `:external-user` - so we create a role binding for this user:\n\n\n```bash\nkubectl --kubeconfig admin.kubeconfig apply -f- &lt;&lt;EOF\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: dex:admin\nrules:\n- apiGroups: [\"\"]\n  resources:\n    - configmaps\n  verbs:\n    - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: dex:admin\nsubjects:\n- kind: User\n  name: admin@example.com:external-user\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: dex:admin\n  apiGroup: rbac.authorization.k8s.io\nEOF\n</code></pre></p> <p>If we now try to access the configmap again with the oidc kubeconfig we are able to read it:</p> <pre><code>kubectl --kubeconfig oidc.kubeconfig.yaml get configmap hello-world -o yaml\n</code></pre> <p>And we still cannot delete existing configmaps:</p> <p>```bash noci kubectl --kubeconfig oidc.kubeconfig.yaml delete configmap hello-world <pre><code>... or create new ones:\n\n```bash noci\nkubectl --kubeconfig oidc.kubeconfig.yaml create configmap this-errors \\\n    --from-literal=message=\"This will not work\"\n</code></pre></p>"},{"location":"examples/oidc/readme/#cleanup","title":"Cleanup","text":"<p>Stop the docker containers, delete the network and delete the files:</p> <pre><code>docker stop kcp dex\ndocker network rm kcp\nrm -f authentication-config.yaml\nrm -f dex/server.crt dex/server.key\nrm -f oidc.kubeconfig.yaml oidc.token apiserver.crt\n</code></pre>"},{"location":"learning/","title":"Learning","text":"<p>Collection of learning material from kcp demo sessions.</p> <p>Past learning materials:</p> <ul> <li>KubeCon 2024 Paris: KCP ML Shop demo from session Why Kubernetes Is Inappropriate for Platforms, and How to Make It Better.</li> <li>KubeCon 2024 Saltlake City Mounts VirtualWorkspace demo from session Deep dive into Generic Control Planes and kcp</li> <li>KubeCon 2025 London: Basics with kcp, and making your own DBaaS from session Tutorial: Exploring Multi-Tenant Kubernetes APIs and Controllers With kcp</li> </ul>"},{"location":"learning/20240321-kubecon-paris/","title":"KCP ML Shop demo","text":"<p>This is a demo of a simple machine-learning service, hosted on KCP. It shows how to use the KCP as a platform to deploy a  machine-learning model and expose it as a REST API. Use global shards to run models close to data (geo placement).</p> <p>Demo recording: https://youtu.be/7op_r9R0fCo?si=8KxUnEgSASBRJCFG&amp;t=1515 </p>"},{"location":"learning/20240321-kubecon-paris/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>KCP running with shards - shards <code>root</code> and <code>beta</code> are required. </li> <li>See KCP sharded helm example</li> <li>Kubectl installed</li> <li>Kubectl-workspace plugin installed</li> </ul>"},{"location":"learning/20240321-kubecon-paris/#demo-scenario","title":"Demo scenario","text":""},{"location":"learning/20240321-kubecon-paris/#platform-operator-persona","title":"Platform operator persona","text":"<p>The platform operator is responsible for setting up the platform and enabling 3rd party. This is done by creating APIExports and APIBindings in dedicated workspace. Workspace management is done via <code>kubectl-workspace</code> plugin.</p> <pre><code>export KUBECONFIG=kcp.kubeconfig\n</code></pre> <ol> <li>Show current \"platform backend\" view:</li> </ol> <pre><code>k ws tree\n</code></pre> <ol> <li>Bootstrap the assets. This shows the platforms provider enabling 3rd party to handle the management of certain resources via APIBindings:</li> </ol> <pre><code>go run ./cmd/controller/ init --kubeconfig=kcp.kubeconfig\nk ws tree\n</code></pre> <ol> <li>Show what server side looks like:</li> </ol> <pre><code>k ws use root:ml:training\nk get apiexports\nk get apiexport -o yaml\n</code></pre> <p>Now that a platform owner has pre-created assets, and 3rd party can use them to deploy models, let us move to service provider persona. </p> <p>Important: In the demo we used the same <code>kcp.kubeconfig</code> for both platform operator and service provider. In real-world scenario, those would be different kubeconfigs, preferably generated by the platform operator and certificate-based. </p>"},{"location":"learning/20240321-kubecon-paris/#service-provider-persona","title":"Service provider persona","text":"<ol> <li>The service provider has a controller to run pointing to this APIExport:</li> </ol> <pre><code>export KUBECONFIG=idp.kubeconfig\n</code></pre> <p>Important: Mount code is not yet open-sourced and it's a work in progress. The cluster running the controller has to have access to both APIExport exported URLs, showed in the previous step. In a real-world scenario, this would be done via VPN or other secure means, where shards can be accessed.</p> <ol> <li>Deploy the controller:</li> </ol> <pre><code>k apply -f ../kcp-ml-shop/demo/deploy/namespace.yaml\nk create secret generic kubeconfig --from-file=kcp.kubeconfig -n ml\nk apply -f ../kcp-ml-shop/demo/deploy/deploy.yaml\n</code></pre>"},{"location":"learning/20240321-kubecon-paris/#user-persona","title":"User persona","text":"<ol> <li>User has a model to deploy - Create an ML-enabled workspace (in 2 locations):</li> </ol> <pre><code>k ws create ml-europe --location-selector=name=root --type ml-training\nk ws create ml-us --location-selector=name=beta --type ml-training\n</code></pre> <ol> <li>Deploy the model</li> </ol> <pre><code>cat demo/model.yaml\nk ws use ml-europe\nk create -f demo/model.yaml\n# get to US:\nk ws use ..\nk ws use ml-us\nk create -f demo/model.yaml\n</code></pre>"},{"location":"learning/20241013-kubecon-saltlakecity/mounts-vw/","title":"Remote mounts virtual workspace example","text":"<p>This is an example of how to use the mounts feature in kcp to mount a target cluster into a workspace. Mount happens at front-proxy level via reverse proxy. You need to implement your own VirtualWorkspace to handle this. and this example shows how to do it.</p>"},{"location":"learning/20241013-kubecon-saltlakecity/mounts-vw/#this-is-example-codebase-and-should-be-used-only-as-example","title":"This is example codebase, and should be used only as example.","text":""},{"location":"learning/20241013-kubecon-saltlakecity/mounts-vw/#step-by-step-guide","title":"Step by step guide","text":"<p>Step by step guide how to setup this example.</p> <ol> <li>Start kcp with mounts feature gate enabled:</li> </ol> <pre><code> go run ./cmd/kcp start --miniproxy-mapping-file=../contrib/20241013-kubecon-saltlakecity/mounts-vw/assets/path-mapping.yaml --feature-gates=WorkspaceMounts=true \n</code></pre> <ol> <li>Setup all required workspaces and exports for virtual workspace to run:</li> </ol> <p>Provider workspace where all the target cluster will be defined with secrets. These clusters can be mounted later on by the any workspace.</p> <p>Setup providers:</p> <pre><code># Set kcp KUBECONFIG\nexport KUBECONFIG=../../../kcp/.kcp/admin.kubeconfig     \n\nkubectl ws use :root\n# create provider workspaces\nkubectl ws create providers --enter\nkubectl ws create mounts --enter\n\n# create exports\nkubectl create -f config/mounts/resources/apiresourceschema-targetkubeclusters.targets.contrib.kcp.io.yaml\nkubectl create -f config/mounts/resources/apiresourceschema-kubeclusters.mounts.contrib.kcp.io.yaml\nkubectl create -f config/mounts/resources/apiresourceschema-targetvclusters.targets.contrib.kcp.io.yaml\nkubectl create -f config/mounts/resources/apiresourceschema-vclusters.mounts.contrib.kcp.io.yaml\nkubectl create -f config/mounts/resources/apiexport-mounts.contrib.kcp.io.yaml\nkubectl create -f config/mounts/resources/apiexport-targets.contrib.kcp.io.yaml\n</code></pre> <ol> <li> <p>Start virtual workspace process: <pre><code> go run ./cmd/virtual-workspaces/ start \\\n --kubeconfig=../../../kcp/.kcp/admin.kubeconfig      \\\n --tls-cert-file=../../../kcp/.kcp/apiserver.crt \\\n --tls-private-key-file=../../../kcp/.kcp/apiserver.key \\\n --authentication-kubeconfig=../../../kcp/.kcp/admin.kubeconfig \\\n --virtual-workspaces-proxy-hostname=https://localhost:6444 \\\n -v=8\n</code></pre></p> </li> <li> <p>Continue bootstrapping the mounts example:</p> </li> </ol> <pre><code># create operators namespace where platforms operators will create objects. This could be many of them.\n# for this example we will use only one.\n\nkubectl ws use :root\nkubectl ws create operators --enter\nkubectl ws create mounts --enter\n\n# bind the exports\nkubectl kcp bind apiexport root:providers:mounts:targets.contrib.kcp.io  --accept-permission-claim secrets.core\n\n# Create a target cluster to `kind` cluster locally:\n\n# create kind cluster if not already created\nkind create cluster --name kind --kubeconfig kind.kubeconfig\n\n#create secret with kubeconfig:\nkubectl ws use :root:operators:mounts\nkubectl create secret generic kind-kubeconfig --from-file=kubeconfig=kind.kubeconfig\n\n# create target cluster:\nkubectl create -f config/mounts/resources/example-target-cluster.yaml\n\n# create vcluster target:\nkubectl create -f config/mounts/resources/example-target-vcluster.yaml\n\n# get secret string:\nkubectl get TargetKubeCluster proxy-cluster -o jsonpath='{.status.secretString}'\nkTPlAYLMjKJDRly5\n\nNis8xbLPoqPUrapA%\n\nkubectl get TargetVCluster proxy-cluster -o jsonpath='{.status.secretString}'\nNis8xbLPoqPUrapA\n\n# Create a consumer workspace for mounts:\nkubectl ws use :root\nkubectl ws create consumer --enter\n\n# This is not yet available in CLI\nkubectlcreate -f config/mounts/resources/example-workspace-kind-cluster-mounted.yaml   \n\nkubectl kcp bind apiexport root:providers:mounts:mounts.contrib.kcp.io \n\n# !!!!! replace secrets string first in the file bellow :\nkubectl create -f config/mounts/resources/example-mount-cluster.yaml\n</code></pre> <ol> <li>Check the mounts reconciler logs: Now workspace should be backed by mountpoint from front-proxy:</li> </ol> <pre><code>kubectl ws use kind-cluster\nk get pods -A\nNAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE\nkube-system          coredns-7db6d8ff4d-4l625                     1/1     Running   0          22h\nkube-system          coredns-7db6d8ff4d-ntf95                     1/1     Running   0          22h\nkube-system          etcd-kind-control-plane                      1/1     Running   0          22h\nkube-system          kindnet-vv872                                1/1     Running   0          22h\nkube-system          kube-apiserver-kind-control-plane            1/1     Running   0          22h\nkube-system          kube-controller-manager-kind-control-plane   1/1     Running   0          22h\nkube-system          kube-proxy-lkv29                             1/1     Running   0          22h\nkube-system          kube-scheduler-kind-control-plane            1/1     Running   0          22h\nlocal-path-storage   local-path-provisioner-988d74bc-dqnd7        1/1     Running   0          22h\n</code></pre>"},{"location":"learning/20241013-kubecon-saltlakecity/mounts-vw/#vclusters-example","title":"Vclusters example","text":"<p>vCluster are backed by vCluster mounts. This is a way to create a virtual cluster that is backed by a real cluster. You can either provide a kubeconfig or a target cluster to back the vCluster or secretString for \"target\" in the system.</p> <p>kubectl create -f config/mounts/resources/example-workspace-vcluster-mounted.yaml</p> <p>kubectl create -f config/mounts/resources/example-mount-vcluster.yaml</p>"},{"location":"learning/20241013-kubecon-saltlakecity/mounts-vw/#known-issues","title":"Known issues","text":"<ol> <li> <p><code>TargetKubeCluster</code> changes do not propagate to <code>KubeCluster</code> need to wire them up. Challenge is that when these 2 objects are in separate bindings, its more machinery to make them work together.</p> </li> <li> <p>VirtualWorkspace is not yet fully shards aware. Ideally it should be 1 per each shard, and handle only its own workspaces.</p> </li> <li> <p>KubeCluster changes not applied to Workspaces. This might be a bug in core. Need to validate.</p> </li> </ol>"},{"location":"learning/20241013-kubecon-saltlakecity/mounts-vw/reconciler/mounts/","title":"Mounts reconciler","text":"<p>Mounts reconciles the mount in the specific workspace and does the following:</p> <ul> <li>for now if it find secret in store - sets it ready</li> </ul>"},{"location":"learning/20241013-kubecon-saltlakecity/mounts-vw/reconciler/mounts/#vcluster-controller","title":"VCluster controller","text":"<p>Order and flow: 1. <code>Finalizer</code> and sets finaziler and stop &amp; requeue if change was done. (metadata change) 2. <code>Delegated/Direct</code> and sets <code>mountsv1alpha1.ClusterSecretReady,</code> (status change) and passthrough. 3. <code>Provisioner</code> and sets <code>mountsv1alpha1.ClusterReady,</code> (status change) and passthrough. 4. <code>Deprovisioner</code> and sets <code>mountsv1alpha1.ClusterNotReady,</code> (status change) and passthrough.</p>"},{"location":"learning/20241013-kubecon-saltlakecity/mounts-vw/reconciler/targets/","title":"Targets reconciler","text":"<p>Targets reconciles the targets in the specific workspace and does the following:</p> <ul> <li>checks if secret is present</li> <li>check if the target cluster is healthy</li> <li>generates random secret string to be used by mounts</li> </ul>"},{"location":"learning/20250401-kubecon-london/workshop/","title":"KubeCon 2025 London: kcp workshop","text":"<p>We wish to welcome you to KubeCon 2025 London, at kcp's workshop titled Exploring Multi-Tenant Kubernetes APIs and Controllers with kcp!</p> <p>While Kubernetes transformed container orchestration, creating multi-tenant platforms remains a significant challenge. kcp goes beyond DevOps and workload management, to reimagine how we deliver true SaaS experiences for platform engineers. Think workspaces and multi-tenancy, not namespaces in a singular cluster. Think sharding and horizontal scaling, not overly large and hard to maintain deployments. With innovative approaches to well-established building blocks in Kubernetes API-Machinery, this CNCF sandbox project gives you a framework to host and consume any kind of API you need to support your platforms.</p> <p>In this hands-on workshop, you will learn how to extend Kubernetes with kcp, build APIs, and design controllers to tackle multi-tenancy challenges. By exploring real-world scenarios like DBaaS across clusters, you will gain practical skills to create scalable, multi-tenant platforms.</p> <ul> <li>www.kcp.io</li> <li>github.com/kcp-dev/kcp</li> </ul>"},{"location":"learning/20250401-kubecon-london/workshop/#session-outline","title":"Session outline","text":"<ol> <li>Introduction. Together we'll see what SaaS means in the context of Kubernetes, and how kcp plays a key role in enabling platform engineers and developers to build such SaaS platforms.</li> <li>Exploration. We\u2019ll get familiar with basic kcp concepts.</li> <li>Demonstration. We\u2019ll walk through practical examples and guide you through the prepared exercises.</li> <li>Execution. Together, we\u2019ll put everything we\u2019ve learnt into action and build a tiny DBaaS platform\u2014right on your PC!</li> </ol> <p>The examples will touch on hosting and consuming SaaS-like APIs we'll create during the session: self-servicing databases to be used in a web-app platform.</p>"},{"location":"learning/20250401-kubecon-london/workshop/#before-we-begin","title":"Before we begin","text":"<p>Before we begin, we needed to make a few assumptions about your PC and its environment:</p> <ul> <li>Your amd64 or arm64 PC is running recent Linux with systemd or MacOS; alternatively you may use GitHub Codespaces or Google Cloud Console.</li> <li>We are also expecting you have installed Docker or podman, and git; if not, please do it now.</li> </ul> <p>For attendees at the conference: to minimize variance between everyone's work environments, thank you for preferring GitHub Codespaces or Google Cloud Console!</p> <p>All the tools and services we'll present during this workshop are local-installation only, don't require super-user privileges, and won't make permanent changes to your system.</p>"},{"location":"learning/20250401-kubecon-london/workshop/#starting-out","title":"Starting out","text":"<p>Once ready, start by heading over to the first warm-up exercise: <code>00-prerequisites</code> \ud83d\udd25!</p>"},{"location":"learning/20250401-kubecon-london/workshop/00-prerequisites/","title":"Pre-requisites","text":"<p>In this chapter we'll set up our workshop-dedicated development environment.</p>"},{"location":"learning/20250401-kubecon-london/workshop/00-prerequisites/#cloning-the-workshop-repo","title":"Cloning the workshop repo","text":"<p>Start by cloning the git repository we'll refer to throughout the workshop, and will be the place for the binaries, scripts and kubeconfigs we will create as we move forward.</p> <p>Important: We will need 4 terminal windows for long running programs &amp; interactions to the same underlying machine during this workshop.</p> <pre><code>git clone https://github.com/kcp-dev/contrib.git kcp-contrib\ncd kcp-contrib/20250401-kubecon-london/workshop\n</code></pre> <p>Now, let's see what's inside.</p> <ul> <li><code>00-prerequisites/</code></li> <li><code>01-deploy-kcp/</code></li> <li><code>02-explore-workspaces/</code></li> <li><code>03-dynamic-providers/</code></li> <li><code>clean-all.sh</code></li> </ul> <p>Notice the exercises in directories <code>&lt;Sequence number&gt;-&lt;Exercise name&gt;</code>. These are the rules:</p> <ol> <li>exercises need to be visited in sequence. To complete one, all previous exercises need to be completed first.</li> <li>Are you stuck? While it's best if you try to follow the tasks by yourself, if you ever get stuck, you can finish an exercise by running the scripts inside the respective exercise directory.</li> <li>Something broke? If you ever need to reset, run <code>clean-all.sh</code> to clean up.</li> <li>Finished an exercise? High-five! Each exercise directory has a script <code>99-highfive.sh</code>. Run it to check-in your progress!</li> </ol>"},{"location":"learning/20250401-kubecon-london/workshop/00-prerequisites/#get-your-bins","title":"Get your bins","text":"<p>Ready for a warm-up? In this quick exercise we are going to install programs we'll be using:</p> <ul> <li>kcp,</li> <li>kcp's api-syncagent,</li> <li>kcp's multicluster-controller runtime example binary,</li> <li>kind,</li> <li>kubectl,</li> <li>and, kubectl-krew.</li> </ul> <p>Install them all in one go by running the following script:</p> <pre><code>00-prerequisites/01-install.sh\n</code></pre> <p>Inspect it first, and you'll see that it <code>curl</code>s files from the GitHub releases of the respective project repositories, and stores them in <code>bin/</code>, inside our current working directory.</p> <p>Alternatively, you may install the binaries manually. If you already have some of them installed and available in your <code>$PATH</code>, you may skip them--just make sure they are up-to-date. If you choose to go the manual way, please make sure the file names are stripped of any OS and arch names they may contain (e.g. <code>mv kubectl-krew-linux_amd64 kubectl-krew</code>), as we'll refer to them using their system-agnostic names later on.</p> <p>And that's it!</p>"},{"location":"learning/20250401-kubecon-london/workshop/00-prerequisites/#high-five","title":"High-five! \ud83d\ude80\ud83d\ude80\ud83d\ude80","text":"<p>Done already? High-five! Check-in your completion with:</p> <pre><code>00-prerequisites/99-highfive.sh\n</code></pre> <p>If there were no errors, you may continue with the next exercise \ud83d\udd25!</p>"},{"location":"learning/20250401-kubecon-london/workshop/01-deploy-kcp/","title":"Deploy kcp","text":"<p>kcp may be deployed via a Helm chart, an operator, or as a standalone process running on the host. Each of them has its uses as well as advantages and disadvantages. While the most preferable way to deploy kcp is using its dedicated operator, for the sake of simplicity, we've taken the liberty of making the choice for you :) .</p>"},{"location":"learning/20250401-kubecon-london/workshop/01-deploy-kcp/#starting-kcp-as-a-standalone-process","title":"Starting kcp as a standalone process","text":"<p>Important</p> <p>Attention: during these exercises, we'll be making heavy use of environment variables. We will be switching back and forth between clusters, as well as needing access to the binaries we've set up in the previous chapter. Whenever you see this block, it means we are switching an environment. Make sure you <code>cd</code> into the workshop git repo, and copy-paste the commands to your terminal. Let's give it a try!</p> <p>Important</p> Bash/ZSHFish <pre><code>export WORKSHOP_ROOT=\"$(git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\"\nexport PATH=\"${WORKSHOP_ROOT}/bin:${PATH}\"\n</code></pre> <pre><code>set -gx WORKSHOP_ROOT (git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\nset -gx PATH $WORKSHOP_ROOT/bin $PATH\n</code></pre> <p>Starting kcp in standalone mode is as easy as typing <code>kcp start</code> and pressing Enter.</p> <pre><code>cd $WORKSHOP_ROOT &amp;&amp; kcp start\n</code></pre> <p>You should see the program running indefinitely, and outputting its logs--starting with some errors that should clean up in a couple of seconds as the different controllers start up. Leave the terminal window open, as we will keep using this kcp instance throughout the duration of the workshop. In this mode, all kcp's state is in-memory only. That means exiting the process (by, for example, pressing Ctrl+C in this terminal), will lose all its etcd contents.</p> <p>Once kcp's output seems stable, we can start making simple kubectl calls against it. <code>kcp start</code> creates a hidden directory <code>.kcp</code>, where it places its kubeconfig and the certificates.</p> <p>Important</p> <p>Open a second shell and <code>cd</code> into workshop's directory now.</p> Bash/ZSHFish <pre><code>export WORKSHOP_ROOT=\"$(git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\"\nexport PATH=\"${WORKSHOP_ROOT}/bin:${PATH}\"\nexport KUBECONFIG=\"${WORKSHOP_ROOT}/.kcp/admin.kubeconfig\"\n</code></pre> <pre><code>set -gx WORKSHOP_ROOT (git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\nset -gx PATH $WORKSHOP_ROOT/bin $PATH\nset -gx KUBECONFIG $WORKSHOP_ROOT/.kcp/admin.kubeconfig\n</code></pre> <p>The following command should work now:</p> <pre><code>$ kubectl version\nClient Version: v1.32.1\nKustomize Version: v5.5.0\nServer Version: v1.31.0+kcp-v0.26.1\n</code></pre> <p>We'll have a couple more kubeconfigs to switch between, and it will be convenient to have them all in one place. Let's do that now:</p> <pre><code>mkdir -p $WORKSHOP_ROOT/kubeconfigs\nln -s $KUBECONFIG $WORKSHOP_ROOT/kubeconfigs\n</code></pre> <p>And that's it!</p>"},{"location":"learning/20250401-kubecon-london/workshop/01-deploy-kcp/#high-five","title":"High-five! \ud83d\ude80\ud83d\ude80\ud83d\ude80","text":"<p>Finished? High-five! Check-in your completion with:</p> <pre><code>01-deploy-kcp/99-highfive.sh\n</code></pre> <p>If there were no errors, you may continue with the next exercise \ud83d\udd25!</p>"},{"location":"learning/20250401-kubecon-london/workshop/01-deploy-kcp/#cheat-sheet","title":"Cheat-sheet","text":"<p>You may fast-forward through this exercise by running:</p> <ul> <li><code>01-deploy-kcp/01-start-kcp.sh</code> in a new terminal</li> </ul>"},{"location":"learning/20250401-kubecon-london/workshop/02-explore-workspaces/","title":"Explore workspaces","text":"<p>Workspaces are one of kcp's core concepts, and in this exercise we'll explore what they are and how to work with them.</p> <p>See Workspaces documentation at docs.kcp.io/kcp/main/concepts/workspaces/.</p>"},{"location":"learning/20250401-kubecon-london/workshop/02-explore-workspaces/#pre-requisites-take-two","title":"Pre-requisites, take two","text":"<p>Workspaces, or kcp for that matter, is not something that vanilla kubectl knows about. kcp brings support for those using krew plugins. You may remember, we installed kubect-krew in the very first warm-up exercise. Now we need to install the plugins themselves:</p> <p>Important</p> Bash/ZSHFish <pre><code>export WORKSHOP_ROOT=\"$(git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\"\nexport KREW_ROOT=\"${WORKSHOP_ROOT}/bin/.krew\"\nexport PATH=\"${WORKSHOP_ROOT}/bin/.krew/bin:${WORKSHOP_ROOT}/bin:${PATH}\"\nexport KUBECONFIG=${WORKSHOP_ROOT}/kubeconfigs/admin.kubeconfig\n</code></pre> <pre><code>set -gx WORKSHOP_ROOT (git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\nset -gx KREW_ROOT $WORKSHOP_ROOT/bin/.krew\nset -gx PATH $WORKSHOP_ROOT/bin/.krew/bin $WORKSHOP_ROOT/bin $PATH\nset -gx KUBECONFIG $WORKSHOP_ROOT/kubeconfigs/admin.kubeconfig\n</code></pre> <pre><code>kubectl krew index add kcp-dev https://github.com/kcp-dev/krew-index.git\nkubectl krew install kcp-dev/kcp\nkubectl krew install kcp-dev/ws\nkubectl krew install kcp-dev/create-workspace\n# IMPORTANT HACK: https://github.com/kubernetes-sigs/krew/issues/865\ncp $(which kubectl-create_workspace) $KREW_ROOT/bin/kubectl-create-workspace\n</code></pre> <p>Now you should be able to run and inspect these commands: <pre><code>$ kubectl create workspace --help\nCreates a new workspace\n\nUsage:\n  create [flags]\n...\n\n$ kubectl ws --help\nManages KCP workspaces\n\nUsage:\n  workspace [create|create-context|use|current|&lt;workspace&gt;|..|.|-|~|&lt;root:absolute:workspace&gt;] [flags]\n  workspace [command]\n...\n\n$ kubectl kcp --help\n...\n</code></pre></p> <p>With that, let's create some workspaces!</p>"},{"location":"learning/20250401-kubecon-london/workshop/02-explore-workspaces/#sprawling-workspaces","title":"Sprawling workspaces","text":"<p>We'll be using <code>kubectl create workspace</code> command:</p> <pre><code>kubectl create workspace one\nkubectl create workspace two\nkubectl create workspace three --enter\nkubectl create workspace potato\n</code></pre> <p>Now, let's list what we've created:</p> <pre><code>kubectl ws use :\nkubectl get ws\n</code></pre> <p>We haven't seen <code>ws use</code> yet. Using this command you move into a different workspace in the tree of workspaces, much like <code>cd</code> moves you into a different directory described by a path. In the case of workspaces, a path too may be relative or absolute, where <code>:</code> is the path separator, and <code>:</code> alone denotes the root of the tree.</p> <pre><code>kubectl ws use :\nkubectl ws use one\nkubectl get configmap\nkubectl create configmap test --from-literal=test=one\nkubectl get configmap test -o json\n</code></pre> <pre><code>kubectl ws use root:two\nkubectl get configmap\nkubectl create configmap test --from-literal=test=two\nkubectl get configmap test -o json\n</code></pre> <p>Notice how even though these two ConfigMaps have the same name <code>test</code>, and are in the same namespace <code>default</code>, they are actually two distinct objects. They live in two different workspaces, and are completely separate. Workspaces represent logical separation of resources in the cluster.</p> <p>We've created a few workspaces now, and already it's easy to lose sight of what is where. Say hello to <code>ws tree</code>:</p> <pre><code>kubectl ws use :\nkubectl ws tree\n</code></pre> <p>You should get output similar to this:</p> <pre><code>.\n\u2514\u2500\u2500 root\n    \u251c\u2500\u2500 one\n    \u251c\u2500\u2500 three\n    \u2502   \u2514\u2500\u2500 potato\n    \u2514\u2500\u2500 two\n</code></pre>"},{"location":"learning/20250401-kubecon-london/workshop/02-explore-workspaces/#exporting-and-binding-apis-across-workspaces","title":"Exporting and binding APIs across workspaces","text":"<p>Isolation is nice, but what if you need to share?</p> <p>See docs.kcp.io/kcp/main/concepts/apis/exporting-apis/ for detailed documentation.</p> <p>As you'll see next, sharing in this context will be a very well-defined and constrained relationship of provisioning and consuming. We shall model that relationship using workspaces.</p>"},{"location":"learning/20250401-kubecon-london/workshop/02-explore-workspaces/#service-provider","title":"Service provider","text":"<p>Create <code>providers</code> and <code>providers:cowboys</code> workspaces:</p> <pre><code>kubectl ws use :\nkubectl create workspace providers --enter\nkubectl create workspace cowboys --enter\n</code></pre> <pre><code>$ kubectl ws use :\nCurrent workspace is 'root'.\n$ kubectl ws tree\n.\n\u2514\u2500\u2500 root\n    \u251c\u2500\u2500 one\n    \u251c\u2500\u2500 providers\n    \u2502   \u2514\u2500\u2500 cowboys\n    \u251c\u2500\u2500 three\n    \u2502   \u2514\u2500\u2500 potato\n    \u2514\u2500\u2500 two\n\n$ kubectl ws use :root:providers:cowboys\nCurrent workspace is 'root:providers:cowboys' (type root:universal).\n</code></pre> <p>Now that we're in <code>:root:providers:cowboys</code>, let's create an <code>APIResourceSchema</code> and an <code>APIExport</code>. We'll discuss what are they for next.</p> <pre><code>kubectl create -f $WORKSHOP_ROOT/02-explore-workspaces/apis/apiresourceschema.yaml\nkubectl create -f $WORKSHOP_ROOT/02-explore-workspaces/apis/apiexport.yaml\n</code></pre> <p>Starting with the first one, <code>APIResourceSchema</code>:</p> <pre><code>kubectl get apiresourceschema -o json\n</code></pre> <p>Try to skim through the YAML output and you'll notice that it is almost identical to a definition of a CRD. Unlike a CRD however, <code>APIResourceSchema</code> instance does not have a backing API server, and instead it simply describes an API that we can pass around and refer to. By decoupling the schema definition from serving, API owners can be more explicit about API evolution.</p> <pre><code>kubectl get apiexport cowboys -o yaml\n</code></pre> <p>Take a note of the following properties in the output:</p> <ul> <li><code>.spec.latestResourceSchemas</code>: lists which APIResourceSchemas we are exporting,</li> <li><code>.spec.permissionClaims</code>: describes resource permissions that our API depends on. These are the permissions that we, the service provider, want the consumer to grant us,</li> <li><code>.status.virtualWorkspaces[].url</code>: a Kubernetes endpoint to access all resources that belong to this export, across all consumers.</li> </ul> <pre><code># Stripped down example output of `kubectl get apiexport` command above.\nspec:\n  latestResourceSchemas:\n  - today.cowboys.wildwest.dev\n  permissionClaims:\n  - all: true\n    group: \"\"\n    resource: configmaps\nstatus:\n  virtualWorkspaces:\n  - url: https://192.168.32.7:6443/services/apiexport/1ctnpog1ny8bnud6/cowboys\n</code></pre>"},{"location":"learning/20250401-kubecon-london/workshop/02-explore-workspaces/#service-consumer","title":"Service consumer","text":"<p>With the provider in place, let's shift into the role of a consumer. Actually, two consumers, in their own workspaces! Let's start with the first one, named \"wild-west\":</p> <pre><code>kubectl ws use :\nkubectl create workspace consumers --enter\nkubectl create workspace wild-west --enter\nkubectl kcp bind apiexport root:providers:cowboys:cowboys --name cowboys-consumer --accept-permission-claim configmaps.core\nkubectl create -f $WORKSHOP_ROOT/02-explore-workspaces/apis/consumer-wild-west.yaml\n</code></pre> <p>Let's check the Cowboy we have created:</p> <pre><code>$ kubectl get cowboy buckaroo-bill -o json\n{\n    \"apiVersion\": \"wildwest.dev/v1alpha1\",\n    \"kind\": \"Cowboy\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kcp.io/cluster\": \"2snrfbp1a3gww1hu\"\n        },\n        \"creationTimestamp\": \"2025-03-12T09:06:53Z\",\n        \"generation\": 1,\n        \"name\": \"buckaroo-bill\",\n        \"namespace\": \"default\",\n        \"resourceVersion\": \"3164\",\n        \"uid\": \"bb6ece46-84bc-4673-a926-f38c486799cf\"\n    },\n    \"spec\": {\n        \"intent\": \"Ride and protect the wild west!!!\"\n    }\n}\n</code></pre> <p>And the second consumer, \"wild-north\":</p> <pre><code>kubectl ws use ..\nkubectl create workspace wild-north --enter\nkubectl kcp bind apiexport root:providers:cowboys:cowboys --name cowboys-consumer --accept-permission-claim configmaps.core\nkubectl create -f $WORKSHOP_ROOT/02-explore-workspaces/apis/consumer-wild-north.yaml\n</code></pre> <pre><code>$ kubectl get cowboy hold-the-wall -o json\n{\n    \"apiVersion\": \"wildwest.dev/v1alpha1\",\n    \"kind\": \"Cowboy\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kcp.io/cluster\": \"30j93qa92345q3tp\"\n        },\n        \"creationTimestamp\": \"2025-03-12T09:09:32Z\",\n        \"generation\": 1,\n        \"name\": \"hold-the-wall\",\n        \"namespace\": \"default\",\n        \"resourceVersion\": \"3227\",\n        \"uid\": \"ff96ab88-b738-4af7-8cc0-3872c424d9df\"\n    },\n    \"spec\": {\n        \"intent\": \"North is there the wall is!\"\n    }\n}\n</code></pre> <p>Great! We have created two instances of a common API, and were able to create a couple of dummy objects with it.</p> <pre><code>$ kubectl ws use :\nCurrent workspace is 'root'.\n$ kubectl ws tree\n.\n\u2514\u2500\u2500 root\n    \u251c\u2500\u2500 consumers\n    \u2502   \u251c\u2500\u2500 wild-north\n    \u2502   \u2514\u2500\u2500 wild-west\n    \u251c\u2500\u2500 one\n    \u251c\u2500\u2500 providers\n    \u2502   \u2514\u2500\u2500 cowboys\n    \u251c\u2500\u2500 three\n    \u2502   \u2514\u2500\u2500 potato\n    \u2514\u2500\u2500 two\n</code></pre>"},{"location":"learning/20250401-kubecon-london/workshop/02-explore-workspaces/#spec-up-status-down","title":"Spec up, status down","text":"<p>We have been moving across namespaces up and down, changing our implied roles. Let's become the service provider again, and see what we can make out from our <code>cowboys</code> APIExport.</p> <pre><code>kubectl ws :root:providers:cowboys\nkubectl get apiexport cowboys -o json | jq '.status.virtualWorkspaces[].url'\n</code></pre> <p>Using that URL, we can confirm that we have access to the resources the consumers have agreed to:</p> <pre><code>$ kubectl -s 'https://192.168.32.7:6443/services/apiexport/1ctnpog1ny8bnud6/cowboys/clusters/*' api-resources\nNAME          SHORTNAMES   APIVERSION              NAMESPACED   KIND\nconfigmaps                 v1                      true         ConfigMap\napibindings                apis.kcp.io/v1alpha1    false        APIBinding\ncowboys                    wildwest.dev/v1alpha1   true         Cowboy\n</code></pre> <p>We can also list all consumers (i.e. workspaces that have relevant <code>APIBinding</code>) for cowboys <code>APIExport</code>:</p> <pre><code>$ kubectl -s 'https://192.168.32.7:6443/services/apiexport/1ctnpog1ny8bnud6/cowboys/clusters/*' get cowboys -A\nNAMESPACE   NAME\ndefault     buckaroo-bill\ndefault     hold-the-wall\n</code></pre> <p> </p> <p>You can play around with inspecting the json output of those commands, and try addressing a specific cluster instead of all of them (wildcard <code>*</code>) to get some intuition about how they are wired together.</p> <p>From that, you can already start imagining what a workspace-aware controller operating on these objects would look like: being able to observe global state in its workspace subtree, it would watch spec updates from its children (Spec up), and push them status updates (Status down). Our basic example is lacking such a controller. But that's something we are going to fix the next exercise, on a more interesting example!</p>"},{"location":"learning/20250401-kubecon-london/workshop/02-explore-workspaces/#high-five","title":"High-five! \ud83d\ude80\ud83d\ude80\ud83d\ude80","text":"<p>Finished? High-five! Check-in your completion with:</p> <pre><code>02-explore-workspaces/99-highfive.sh\n</code></pre> <p>If there were no errors, you may continue with the next exercise \ud83d\udd25!</p>"},{"location":"learning/20250401-kubecon-london/workshop/02-explore-workspaces/#cheat-sheet","title":"Cheat-sheet","text":"<p>You may fast-forward through this exercise by running: * <code>02-explore-workspaces/00-install-krew-plugins.sh</code> * <code>02-explore-workspaces/01-create-apis.sh</code> * <code>02-explore-workspaces/02-create-consumers.sh</code> * <code>02-explore-workspaces/99-highfive.sh</code></p>"},{"location":"learning/20250401-kubecon-london/workshop/03-dynamic-providers/","title":"Dynamic providers","text":"<p>In this exercise we'll explore an actual SaaS scenario: self-provisioning PostgreSQL databases, where:</p> <ul> <li>one (external) cluster will be in the role of a service owner, running the database servers,</li> <li>one workspace will be in the role of a service provider, where consumers can self-service their databases,</li> <li>one or more workspaces will be consuming the database(s).</li> </ul> <p>Excited? Let's get down to it!</p>"},{"location":"learning/20250401-kubecon-london/workshop/03-dynamic-providers/#herding-databases","title":"Herding databases \ud83d\udc11\ud83d\udc11\ud83d\udc11","text":"<p>Important</p> Bash/ZSHFish <pre><code>export WORKSHOP_ROOT=\"$(git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\"\nexport EXERCISE_DIR=\"${WORKSHOP_ROOT}/03-dynamic-providers\"\nexport KUBECONFIGS_DIR=\"${WORKSHOP_ROOT}/kubeconfigs\"\nexport KREW_ROOT=\"${WORKSHOP_ROOT}/bin/.krew\"\nexport PATH=\"${WORKSHOP_ROOT}/bin/.krew/bin:${WORKSHOP_ROOT}/bin:${PATH}\"\n</code></pre> <pre><code>set -gx WORKSHOP_ROOT (git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\nset -gx EXERCISE_DIR $WORKSHOP_ROOT/03-dynamic-providers\nset -gx KUBECONFIGS_DIR $WORKSHOP_ROOT/kubeconfigs\nset -gx KREW_ROOT $WORKSHOP_ROOT/bin/.krew\nset -gx PATH $WORKSHOP_ROOT/bin/.krew/bin $WORKSHOP_ROOT/bin $PATH\n</code></pre> <p>Surprise! You've just been appointed as the owner of a company responsible for running PostgreSQL databases, SQL<sup>3</sup> Co.! What's worse, you haven't heard of kcp yet! What you did hear of though is that PostgreSQL servers need compute and storage. And that Kubernetes can do all of that. So, to get things going, let's start up a kind-backed Kubernetes cluster:</p> <p>Creating a kind cluster</p> Dockerpodman <pre><code>kind create cluster --name provider --kubeconfig $KUBECONFIGS_DIR/provider.kubeconfig\n</code></pre> <pre><code>kind create cluster --name provider --kubeconfig $KUBECONFIGS_DIR/provider.kubeconfig\n</code></pre> <p>If that doesn't work (error <code>running kind with rootless provider requires setting systemd property \"Delegate=yes\"</code>), try running the following command instead:</p> <pre><code>systemd-run --user --scope \\\n  --unit=\"workshop-kcp-kind.scope\" \\\n  --property=Delegate=yes \\\n  kind create cluster --name provider --kubeconfig $KUBECONFIGS_DIR/provider.kubeconfig\n</code></pre> <p>See the following links for details: https://kind.sigs.k8s.io/docs/user/rootless/ and https://lists.fedoraproject.org/archives/list/devel@lists.fedoraproject.org/thread/ZMKLS7SHMRJLJ57NZCYPBAQ3UOYULV65/.</p> <p>Once the cluster is created, you can verify it's working with kubectl:</p> <pre><code>$ KUBECONFIG=$KUBECONFIGS_DIR/provider.kubeconfig kubectl version\nClient Version: v1.32.2\nKustomize Version: v5.5.0\nServer Version: v1.32.2\n</code></pre> <p>Success all around, let's deploy the CloudNative PG--the Kubernetes PostgreSQL operator:</p> <pre><code>KUBECONFIG=$KUBECONFIGS_DIR/provider.kubeconfig kubectl apply --server-side -f \\\n    https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.25/releases/cnpg-1.25.1.yaml\n</code></pre> <p>While its Pods are starting up, let's move onto the next step.</p>"},{"location":"learning/20250401-kubecon-london/workshop/03-dynamic-providers/#give-and-take","title":"Give and take","text":"<p>Whew, we're back in kcp land! Equipped with the knowledge from our last exercise, we already know that we can model producer-consumer relationship as workspaces, where one provides an APIExport (that exposes APIs described by APIResourceSchemas), and the other(s) \"imports\" it by binding to it, with an APIBinding.</p> <p>Important</p> Bash/ZSHFish <pre><code>cp ${KUBECONFIGS_DIR}/admin.kubeconfig ${KUBECONFIGS_DIR}/sync-agent.kubeconfig\nexport KUBECONFIG=\"${KUBECONFIGS_DIR}/sync-agent.kubeconfig\"\n</code></pre> <pre><code>cp $KUBECONFIGS_DIR/admin.kubeconfig $KUBECONFIGS_DIR/sync-agent.kubeconfig\nset -gx KUBECONFIG $KUBECONFIGS_DIR/sync-agent.kubeconfig\n</code></pre> <p>Notice that <code>cp</code> call just above, and how we set up the <code>KUBECONFIG</code> variable? While important, we will come back to why we did that later, in the next section. For now, let's focus on creating the provider's workspace, and an APIExport in it. We already have <code>:root:providers</code> workspace created, from where we provisioned Cowboys. While not very useful, we now know the commands to move around in the hierarchy of workspaces, and can create them too! Let's do that, this time for databases:</p> <pre><code>kubectl ws use :root:providers\nkubectl ws create database --enter\n</code></pre> <p>The definition of the APIExport is already prepared for us in <code>$EXERCISE_DIR/apis/export.yaml</code>:</p> <pre><code>kubectl apply -f $EXERCISE_DIR/apis/export.yaml\n</code></pre> <p>If you're curious, you may go ahead and inspect the file and/or the object that was just created, and you'll notice that it's rather empty. As we continue forward, we'll see it populate with actual pgsql database specs and statuses. For now, let's create an equally empty APIBinding, to match our empty APIExport:</p> <p>Important</p> Bash/ZSHFish <pre><code>export KUBECONFIG=\"${KUBECONFIGS_DIR}/admin.kubeconfig\"\n</code></pre> <pre><code>set -gx KUBECONFIG $KUBECONFIGS_DIR/admin.kubeconfig\n</code></pre> <pre><code>kubectl ws :root:consumers\nkubectl ws create pg --enter\nkubectl kcp bind apiexport root:providers:database:postgresql.cnpg.io --accept-permission-claim secrets.core,namespaces.core\n</code></pre> <p>We've created a workspace <code>:root:consumers:pg</code>. The APIExport needs permissions to secrets, as it will store the authentication credentials for the databases we'll create, hence the permission claim flag.</p> <p>With all that done, we're ready to connect the dots: the external cluster running the pgsql servers, the provider workspace exposing the postgresql.cnpg.io APIs, and the consumer workspace self-provisioning the pgsql servers and databases.</p>"},{"location":"learning/20250401-kubecon-london/workshop/03-dynamic-providers/#connect-the-dots","title":"Connect the dots","text":"<p>In the last exercise we discussed how there is nothing to reconcile Cowboys between workspaces, and that we'd need a controller that is able to observe state globally, react on Spec changes and update Status of the watched objects. Moreover, we need not only synchronizing APIs and objects across workspaces, but also across an external Kubernetes cluster that we've created above.</p> <p>To do all of that, kcp offers one implementation of such a controller, the api-syncagent. The api-syncagent generally runs in the cluster owning the service, i.e. our kind cluster. Then, the service owner would publish the API groups that are to be exposed to kcp--this is done by defining a PublishedResource object which we'll see in a bit. The published resources are then picked up by the api-syncagent, creating APIResourceSchemas for them automatically, and shoving them into the prepared APIExport on the kcp side, making them ready for consumption. There is a lot more going on, and you can consult the project's documentation. But for now, this brief introduction shall suffice and we can move onto incorporating the controller into our seedling infrastructure.</p> <p>Important</p> Bash/ZSHFish <pre><code>export KUBECONFIG=\"${KUBECONFIGS_DIR}/provider.kubeconfig\"\n</code></pre> <pre><code>set -gx KUBECONFIG $KUBECONFIGS_DIR/provider.kubeconfig\n</code></pre> <pre><code>kubectl apply --server-side -f https://raw.githubusercontent.com/kcp-dev/api-syncagent/refs/tags/v0.2.0/deploy/crd/kcp.io/syncagent.kcp.io_publishedresources.yaml\nkubectl apply -f $EXERCISE_DIR/apis/resources-cluster.yaml\nkubectl apply -f $EXERCISE_DIR/apis/resources-database.yaml\n</code></pre> <p>We've created PublishedResource objects for <code>clusters</code> and <code>databases</code> resources of the <code>postgresql.cnpg.io</code> API group. Give yourself a second and check the definitions we've just applied. Take a look at the <code>publish-cnpg-cluster</code> PublishedResource, and you'll notice that it's not publishing just the pgsql Cluster, but also a Secret:</p> <pre><code>  # ... snip ...\n  related:\n  - kind: Secret\n    origin: kcp\n    identifier: credentials\n    reference:\n      name:\n        path: \"spec.superuserSecret.name\"\n</code></pre> <p>Currently in the role of a service owner, we know that pgsql (and specifically cnpg) stores credentials to the database server in a Secret, and that the secret is referenced by the <code>cluster</code> resource in its <code>spec.superuserSecret.name</code> field. The api-syncagent will extract that object using the path we supplied, and share it along with the <code>cluster</code> resource in the APIExport.</p> <p>And that's it! The only thing left for us to do is to run the controller itself. api-syncagent can be deployed inside the cluster, or run externally as a standalone process. To make things simpler, we are going with the latter option. Keep the process running:</p> <pre><code>api-syncagent --namespace default --apiexport-ref postgresql.cnpg.io --kcp-kubeconfig=$KUBECONFIGS_DIR/sync-agent.kubeconfig\n</code></pre> <p>At the very beginning of this exercise we've made a copy of <code>admin.kubeconfig</code> into <code>sync-agent.kubeconfig</code> and using that we've created the <code>:root:providers:database</code> workspace. If you are wondering how does api-syncagent know where it can find the prepared APIExport, this is how. The kubeconfig has its context set to that workspace's endpoint. Now, leave the controller running and let's go create some databases finally!</p> <p>Bonus step</p> <p>If you are curious what happened to our mostly empty APIExport and APIBinding objects from before, now would be the time to <code>KUBECONFIG=$KUBECONFIGS_DIR/admin.config kubectl get -o json</code> them in another terminal. Can you spot what's different?</p>"},{"location":"learning/20250401-kubecon-london/workshop/03-dynamic-providers/#may-i-have-some","title":"May I have some?","text":"<p>Important</p> <p>At this point we have two shells running two processes. Let's open a third shell so we can continue.</p> Bash/ZSHFish <pre><code>export WORKSHOP_ROOT=\"$(git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\"\nexport EXERCISE_DIR=\"${WORKSHOP_ROOT}/03-dynamic-providers\"\nexport KUBECONFIGS_DIR=\"${WORKSHOP_ROOT}/kubeconfigs\"\nexport KREW_ROOT=\"${WORKSHOP_ROOT}/bin/.krew\"\nexport PATH=\"${WORKSHOP_ROOT}/bin/.krew/bin:${WORKSHOP_ROOT}/bin:${PATH}\"\nexport KUBECONFIG=\"${KUBECONFIGS_DIR}/admin.kubeconfig\"\n</code></pre> <pre><code>set -gx WORKSHOP_ROOT (git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\nset -gx EXERCISE_DIR $WORKSHOP_ROOT/03-dynamic-providers\nset -gx KUBECONFIGS_DIR $WORKSHOP_ROOT/kubeconfigs\nset -gx KREW_ROOT $WORKSHOP_ROOT/bin/.krew\nset -gx PATH $WORKSHOP_ROOT/bin/.krew/bin $WORKSHOP_ROOT/bin $PATH\nset -gx KUBECONFIG $KUBECONFIGS_DIR/admin.kubeconfig\n</code></pre> <pre><code>kubectl ws use :root:consumers:pg\n</code></pre> <p>Bam! You've just been promoted to a consumer! You don't have an application to run yet, but you know it will need a database or two for sure. Things couldn't be easier, because your company closed a contract with SQL<sup>3</sup> Co., and what's more, there is an APIBinding in your workspace, importing the postgresql.cnpg.io APIs, ready to use.</p> <pre><code>kubectl apply -f $EXERCISE_DIR/apis/consumer-1-cluster.yaml\nkubectl apply -f $EXERCISE_DIR/apis/consumer-1-database.yaml\n</code></pre> <p>It's important we wait for the resources to be ready before we continue:</p> <pre><code># Notice that the pgsql cluster is still booting up:\n\n$ kubectl get cluster\nNAME   AGE   INSTANCES   READY   STATUS               PRIMARY\nkcp    27s   1                   Setting up primary\n\n# ... 1 to 5 minutes later ...\n\n# This is what a healthy cluster status looks like:\n\n$ kubectl get cluster\nNAME   AGE   INSTANCES   READY   STATUS                     PRIMARY\nkcp    50s   1           1       Cluster in healthy state   kcp-1\n</code></pre> <p>And just like that, we have a PostgreSQL server with a database, that somebody else is running. Try to follow the example below!</p> Service owner<pre><code>$ export KUBECONFIG=$KUBECONFIGS_DIR/provider.kubeconfig\n$ kubectl get namespaces\n1yaxsslokc5aoqme     Active   6m34s\ncnpg-system          Active   10m\ndefault              Active   10m\nkube-node-lease      Active   10m\nkube-public          Active   10m\nkube-system          Active   10m\nlocal-path-storage   Active   10m\n\n$ kubectl -n 1yaxsslokc5aoqme get clusters\nNAME   AGE     INSTANCES   READY   STATUS                     PRIMARY\nkcp    7m46s   1           1       Cluster in healthy state   kcp-1\n$ kubectl -n 1yaxsslokc5aoqme get databases\nNAME     AGE    CLUSTER   PG NAME   APPLIED   MESSAGE\ndb-one   8m3s   kcp       one       true\n</code></pre> Service consumer<pre><code>$ export KUBECONFIG=$KUBECONFIGS_DIR/admin.kubeconfig\n$ kubectl ws use :root:consumers:pg\nCurrent workspace is 'root:consumers:pg' (type root:universal).\n\n$ kubectl get databases\nNAME     AGE     CLUSTER   PG NAME   APPLIED   MESSAGE\ndb-one   8m45s   kcp       one       true\n\n$ kubectl get clusters\nNAME   AGE     INSTANCES   READY   STATUS                     PRIMARY\nkcp    8m51s   1           1       Cluster in healthy state   kcp-1\n\n$ kubectl get secrets\nNAME            TYPE                       DATA   AGE\nkcp-postgres    kubernetes.io/basic-auth   2      9m3s\nkcp-superuser   kubernetes.io/basic-auth   2      9m3s\n</code></pre> <p>Indeed, if you check the kcp side, you'll see that we have only one consumer <code>pg</code> with a single database instance in our workspace <code>root:consumers:pg</code>. Nothing stops us from creating more however. We are however going to limit ourselves to only one consumer during the workshop. Feel free to explore and create more consumers later yourself!</p> <p> </p> <p>Now, what can we do with it? You may recall that there were Secrets involved in the permission claims when we bound the APIExport. As it turns out, we have a Secret with admin access to the PostgreSQL server (as we should, we own it!), and can use it to authenticate.</p> <p>A side note: we are going to cheat a bit now. We are running all the clusters on the same machine, and we know what IPs and ports to use. Having the username and the password to the DB is one thing, knowing where to connect is another. In the real world, SQL<sup>3</sup> Co. would have created a proper Ingress with a Service for us, and generated a connection string inside a Secret, and this would all work as it stands. Not having done that though, let's agree on a simplification: in place of ingress we will use port-forwarding, and the connection string we will create ourselves.</p> port forward<pre><code>$ export KUBECONFIG=$KUBECONFIGS_DIR/provider.kubeconfig\n$ # Pretending to have an Ingress by port-forwarding in a separate terminal.\n$ # Note that the \"kcp-rw\" service is created by the cnpg operator, and that the \"kcp\" in the name comes from the PostgreSQL cluster name.\n$ kubectl -n 1yaxsslokc5aoqme port-forward svc/kcp-rw 5432:5432\nForwarding from 127.0.0.1:5432 -&gt; 5432\nForwarding from [::1]:5432 -&gt; 5432\nHandling connection for 5432\nHandling connection for 8080\nHandling connection for 8080\nHandling connection for 8080\nHandling connection for 8080\nHandling connection for 8080\nHandling connection for 8080\nHandling connection for 8080\nHandling connection for 8080\n</code></pre> psql client<pre><code>$ export KUBECONFIG=$KUBECONFIGS_DIR/admin.kubeconfig\n$ export pg_username=\"$(kubectl get secret kcp-superuser -o jsonpath='{.data.username}' | base64 -d)\"\n$ export pg_password=\"$(kubectl get secret kcp-superuser -o jsonpath='{.data.password}' | base64 -d)\"\n$ docker run -it --rm --network=host --env PGPASSWORD=$pg_password postgres psql -h 127.0.0.1 -d one -U postgres\npsql (17.4 (Debian 17.4-1.pgdg120+2))\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, compression: off, ALPN: postgresql)\nType \"help\" for help.\n\none=# SELECT * FROM pg_catalog.pg_tables WHERE schemaname = 'pg_catalog';\n schemaname |        tablename         | tableowner | tablespace | hasindexes | hasrules | hastriggers | rowsecurity\n------------+--------------------------+------------+------------+------------+----------+-------------+-------------\n pg_catalog | pg_statistic             | postgres   |            | t          | f        | f           | f\n pg_catalog | pg_type                  | postgres   |            | t          | f        | f           | f\n...\n</code></pre> <p>How cool is that!</p> <p>In this exercise we've seen multiple personas interacting with each other, and each having different responsibilities. This is what the Software-as-a-Service style of workflow can look like. Want more? In the next, and final exercise of this workshop, we'll explore what it's like to develop and deploy mutli-cluster applications against kcp.</p>"},{"location":"learning/20250401-kubecon-london/workshop/03-dynamic-providers/#high-five","title":"High-five! \ud83d\ude80\ud83d\ude80\ud83d\ude80","text":"<p>Finished? High-five! Check-in your completion with:</p> <pre><code>03-dynamic-providers/99-highfive.sh\n</code></pre> <p>If there were no errors, you may continue with the next exercise \ud83d\udd25!</p>"},{"location":"learning/20250401-kubecon-london/workshop/03-dynamic-providers/#cheat-sheet","title":"Cheat-sheet","text":"<p>You may fast-forward through this exercise by running:</p> <ul> <li><code>03-dynamic-providers/00-run-provider-cluster.sh</code></li> <li><code>03-dynamic-providers/01-deploy-postgres.sh</code></li> <li><code>03-dynamic-providers/02-create-provider.sh</code></li> <li><code>03-dynamic-providers/03-create-consumer.sh</code></li> <li><code>03-dynamic-providers/04-run-api-syncagent.sh</code> in a separate terminal</li> <li><code>03-dynamic-providers/05-create-database.sh</code></li> <li><code>03-dynamic-providers/99-highfive.sh</code></li> </ul>"},{"location":"learning/20250401-kubecon-london/workshop/04-application-providers/","title":"Application providers","text":"<p>In our last exercise of this workshop we'll take a look at a kcp-native application that uses sigs.k8s.io/multicluster-runtime to run a kcp provider, and reconciles the deployment across workspaces.</p>"},{"location":"learning/20250401-kubecon-london/workshop/04-application-providers/#going-native","title":"Going native","text":"<p>After the great kick-off with the PostgreSQL-as-a-Service business, the folks back at SQL<sup>3</sup> Co. have decided to give in, and give their kcp-aware customers a treat. They realised that some of the users don't really like the <code>psql</code> CLI interface, and would prefer web-based pgAdmin instead. And so they invested into building a kcp-native pgAdmin provider, with the same principles as we've seen up until now: the workloads stay with the service owner, the spec is consumer's.</p> <p>To accomplish that, they say all they had to use was:</p> <ul> <li>kubebuilder to scaffold the project and CRDs,</li> <li>sigs.k8s.io/multicluster-runtime to provide multicluster manager, controller and reconciler functionalities,</li> <li>github.com/kcp-dev/multicluster-provider to interact with kcp virtual workspaces,</li> <li>github.com/kcp-dev/kcp/sdk to add in kcp schemas,</li> <li>and lastly, github.com/cloudnative-pg/cloudnative-pg to be able to work with postgresql.cnpg.io resources.</li> </ul> <p>We won't go into any implementation details here, but you are very welcome to inspect and play around with the code yourself at https://github.com/kcp-dev/contrib/tree/main/20250401-kubecon-london/workshop/kcp-multicluster-provider-example. The kcp-aware bits are clearly marked to see what multicluster bits need to be added into the kubebuilder-generated base. As far as the complexity goes, we hope you will find it quite underwhelming :)</p>"},{"location":"learning/20250401-kubecon-london/workshop/04-application-providers/#there-is-an-app-in-my-ws","title":"There is an App in my WS! \ud83e\udd0c","text":"<p>The Application brought to you by SQL<sup>3</sup> Co. has a CRD definition and a controller that comes with it, and that they are running (we'll see how right after this) on their infrastructure. They were also nice enough to prepare an APIResourceSchema too! For us at kcp land though, not much will change since the last time. In a provider workspace we create an APIExport for the prepared APIResourceSchema, and then we add consumers by binding to that export.</p> <p>Important</p> Bash/ZSHFish <pre><code>export WORKSHOP_ROOT=\"$(git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\"\nexport EXERCISE_DIR=\"${WORKSHOP_ROOT}/04-application-providers\"\nexport KUBECONFIGS_DIR=\"${WORKSHOP_ROOT}/kubeconfigs\"\nexport KREW_ROOT=\"${WORKSHOP_ROOT}/bin/.krew\"\nexport PATH=\"${WORKSHOP_ROOT}/bin/.krew/bin:${WORKSHOP_ROOT}/bin:${PATH}\"\n\n# Stashing our admin.kubeconfig away for when we deploy the multicluster provider.\ncp ${KUBECONFIGS_DIR}/admin.kubeconfig ${KUBECONFIGS_DIR}/mcp-app.kubeconfig\nexport KUBECONFIG=\"${KUBECONFIGS_DIR}/mcp-app.kubeconfig\"\n</code></pre> <pre><code>set -gx WORKSHOP_ROOT (git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\nset -gx EXERCISE_DIR $WORKSHOP_ROOT/04-application-providers\nset -gx KUBECONFIGS_DIR $WORKSHOP_ROOT/kubeconfigs\nset -gx KREW_ROOT $WORKSHOP_ROOT/bin/.krew\nset -gx PATH $WORKSHOP_ROOT/bin/.krew/bin $WORKSHOP_ROOT/bin $PATH\n\n# Stashing our admin.kubeconfig away for when we deploy the multicluster provider.\ncp $KUBECONFIGS_DIR/admin.kubeconfig $KUBECONFIGS_DIR/mcp-app.kubeconfig\nset -gx KUBECONFIG $KUBECONFIGS_DIR/mcp-app.kubeconfig\n</code></pre> <p>We'll use <code>:root:providers:application</code> workspace for our pgAdmin app export:</p> <pre><code>kubectl ws use :root:providers\nkubectl ws create application --enter\nkubectl apply -f $EXERCISE_DIR/apis/apiresourceschema.yaml\nkubectl apply -f $EXERCISE_DIR/apis/export.yaml\n</code></pre> <p>And now a consumer:</p> <pre><code>kubectl ws :root:consumers:pg\nkubectl kcp bind apiexport root:providers:application:apis.contrib.kcp.io --accept-permission-claim secrets.core\nkubectl apply -f $EXERCISE_DIR/apis/application.yaml\n</code></pre> <p>What does the application look like?</p> <pre><code>apiVersion: apis.contrib.kcp.io/v1alpha1\nkind: Application\nmetadata:\n  name: application-kcp\nspec:\n  databaseRef: db-one\n  databaseSecretRef:\n    name: kcp-superuser\n</code></pre> <p>It references the database we've created earlier, and the Secret with credentials to access it. Meanwhile, a word has got out to SQL<sup>3</sup> Co. that we are ready to use their fancy new Application reconciler, and so they ran this command--in a separate terminal and just left it running:</p> Starting the mcp-app<pre><code>kubectl ws use :root:providers:application\nmcp-example-crd --server=$(kubectl get apiexport apis.contrib.kcp.io -o jsonpath=\"{.status.virtualWorkspaces[0].url}\") \\\n  --provider-kubeconfig $KUBECONFIGS_DIR/provider.kubeconfig\n</code></pre> <p>Important</p> <p>We now have three long-running processes. Let's open a fourth shell now.</p> Bash/ZSHFish <pre><code>export WORKSHOP_ROOT=\"$(git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\"\nexport EXERCISE_DIR=\"${WORKSHOP_ROOT}/04-application-providers\"\nexport KUBECONFIGS_DIR=\"${WORKSHOP_ROOT}/kubeconfigs\"\nexport KREW_ROOT=\"${WORKSHOP_ROOT}/bin/.krew\"\nexport PATH=\"${WORKSHOP_ROOT}/bin/.krew/bin:${WORKSHOP_ROOT}/bin:${PATH}\"\nexport KUBECONFIG=\"${KUBECONFIGS_DIR}/admin.kubeconfig\"\n</code></pre> <pre><code>set -gx WORKSHOP_ROOT (git rev-parse --show-toplevel)/20250401-kubecon-london/workshop\nset -gx EXERCISE_DIR $WORKSHOP_ROOT/04-application-providers\nset -gx KUBECONFIGS_DIR $WORKSHOP_ROOT/kubeconfigs\nset -gx KREW_ROOT $WORKSHOP_ROOT/bin/.krew\nset -gx PATH $WORKSHOP_ROOT/bin/.krew/bin $WORKSHOP_ROOT/bin $PATH\nset -gx KUBECONFIG $KUBECONFIGS_DIR/admin.kubeconfig\n</code></pre> View of the service owner cluster<pre><code>$ export KUBECONFIG=$KUBECONFIGS_DIR/provider.kubeconfig\n$ kubectl get namespaces # Get the namespace in question. Can you guess which one it is?\n$ KUBECONFIG=$KUBECONFIGS_DIR/provider.kubeconfig kubectl -n 1yaxsslokc5aoqme get all\nNAME                                   READY   STATUS    RESTARTS   AGE\npod/application-kcp-578c5dd4df-fwlgw   1/1     Running   0          29s\npod/kcp-1                              1/1     Running   0          10m\n\nNAME                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nservice/application-kcp   ClusterIP   10.96.68.251   &lt;none&gt;        8080/TCP   29s\nservice/kcp-r             ClusterIP   10.96.89.104   &lt;none&gt;        5432/TCP   10m\nservice/kcp-ro            ClusterIP   10.96.69.97    &lt;none&gt;        5432/TCP   10m\nservice/kcp-rw            ClusterIP   10.96.33.11    &lt;none&gt;        5432/TCP   10m\n\nNAME                              READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/application-kcp   1/1     1            1           29s\n\nNAME                                             DESIRED   CURRENT   READY   AGE\nreplicaset.apps/application-kcp--578c5dd4df      1         1         1       29s\n</code></pre> <p> </p> <p>Continuing in our consumer workspace, let's check the Application object!</p> <pre><code>$ kubectl get application application-kcp -o json\n{\n    \"apiVersion\": \"apis.contrib.kcp.io/v1alpha1\",\n    \"kind\": \"Application\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kcp.io/cluster\": \"1yaxsslokc5aoqme\"\n        },\n...\n    \"status\": {\n        \"connectionString\": \"kubectl port-forward svc/application-kcp 8080:8080 -n 1yaxsslokc5aoqme\",\n        \"status\": \"Ready\"\n    }\n}\n</code></pre> <p>Now that's some weird connection string! Similar to what we did in the previous exercise, we didn't want to have our demo setup too complex, and so for the sake of brevity, let's pretend that port forwarding is an actual Ingress, and open up the connection.</p> <pre><code>KUBECONFIG=$KUBECONFIGS_DIR/provider.kubeconfig kubectl port-forward svc/application-kcp 8080:8080 -n 1yaxsslokc5aoqme\n</code></pre>"},{"location":"learning/20250401-kubecon-london/workshop/04-application-providers/#drum-roll","title":"Drum-roll \ud83e\udd41\ud83e\udd41\ud83e\udd41","text":"<p>The last thing for you to do is to open up your browser, and visit <code>localhost:8080</code> in your browser.</p>"},{"location":"learning/20250401-kubecon-london/workshop/04-application-providers/#high-five","title":"High-five! \ud83d\ude80\ud83d\ude80\ud83d\ude80","text":"<p>Congratulations, you've reached the finishing line! Great job!</p> <p>This was a lot to take in, so let's recap. We've gone through basic concepts of kcp, what proper resource isolation looks like, but also how APIs can be shared. We've also learnt that we can specify what resources to share, limiting the scope of what a provider and consumer can reach. Using those principles we've been able to build consumer-producer relationship between Kubernetes endpoints: not only inside a single kcp instance, but with external clusters too. We've also peaked into multicluster controllers and reconcilers, where the options to innovate are vast. We invite you to give it a try, and see how your Kubernetes infrastructure could benefit from a true SaaS approach, with kcp!</p>"},{"location":"learning/20250401-kubecon-london/workshop/04-application-providers/#where-to-next","title":"Where to next","text":"<p>More kcp talks at this conference:</p> <ul> <li>You are here: Tutorial: Exploring Multi-Tenant Kubernetes APIs and Controllers With Kcp - Robert Vasek, Clyso GmbH; Nabarun Pal, Independent; Varsha Narsing, Red Hat; Marko Mudrinic, Kubermatic GmbH; Mangirdas Judeikis, Cast AI</li> <li>Thursday, 3 April 11:00 - 11:30 BST: Extending Kubernetes Resource Model (KRM) Beyond Kubernetes Workloads - Mangirdas Judeikis, Cast AI &amp; Nabarun Pal, Independent</li> <li>also Thursday, 3 April 15:00 - 15:30 BST: Dynamic Multi-Cluster Controllers With Controller-runtime - Marvin Beckers, Kubermatic &amp; Stefan Schimanski, Upbound</li> </ul> <p>After this conference:</p> <ul> <li>Homepage: www.kcp.io</li> <li>The <code>#kcp-dev</code> channel in the Kubernetes Slack workspace.</li> <li>Our mailing lists:<ul> <li>kcp-dev for development discussions.</li> <li>kcp-users for discussions among users and potential users.</li> </ul> </li> <li>By joining the kcp-dev mailing list, you should receive an invite to our bi-weekly community meetings.</li> <li>See recordings of past community meetings on YouTube.</li> <li>The next community meeting dates are available via our CNCF community group.</li> <li>Check the community meeting notes document for future and past meeting agendas.</li> <li>Browse the shared Google Drive to share design docs, notes, etc.<ul> <li>Members of the kcp-dev mailing list can view this drive.</li> </ul> </li> </ul>"}]}